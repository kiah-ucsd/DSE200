{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiah-ucsd/DSE200/blob/Final-Project---Kiah/ML_Workflow_Template_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2679b5b5",
      "metadata": {
        "id": "2679b5b5"
      },
      "source": [
        "# üß† How to Use This Updated EDA Workflow Template\n",
        "\n",
        "This version includes **Numeric vs. Categorical Analysis** before correlation to capture group-based patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485efef1",
      "metadata": {
        "id": "485efef1"
      },
      "source": [
        "# üß≠ Exploratory Data Analysis (EDA) Workflow  \n",
        "*(Action ‚Üí Function ‚Üí Why ‚Üí Next Step ‚Üí Search Rule ‚Üí Visual)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79cb9b0d",
      "metadata": {
        "id": "79cb9b0d"
      },
      "source": [
        "## **1. Import and Preview**\n",
        "**Action:** Load and preview data.  \n",
        "**Functions:** `pd.read_csv()`, `df.head()`, `df.info()`, `df.sample()`  \n",
        "**Why:** Confirm import success, column types, and completeness.  \n",
        "**Next Step:** Merge additional sources if needed.  \n",
        "**Search Rule:** Look for encoding glitches or missing column headers.  \n",
        "**Visual:** üìä *DataFrame snapshot* ‚Äî use `.head()` or `.sample(5)` to display rows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92f2c638",
      "metadata": {
        "id": "92f2c638"
      },
      "source": [
        "## **2. Merge (if multiple sources)**\n",
        "**Action:** Combine related datasets.  \n",
        "**Functions:** `pd.merge()`, `pd.concat()`  \n",
        "**Why:** Integrate profiles, portfolios, or transactions for unified analysis.  \n",
        "**Next Step:** Check data shape and join accuracy.  \n",
        "**Search Rule:** If row count drops significantly, inspect join keys for mismatches.  \n",
        "**Visual:** üîó *Join validation table* ‚Äî e.g.,  \n",
        "```python\n",
        "pd.crosstab(df1['id'].isin(df2['id']), columns='In Both?')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e32af860",
      "metadata": {
        "id": "e32af860"
      },
      "source": [
        "## **3. Basic Overview**\n",
        "**Action:** Assess dataset size, summary stats, and data types.  \n",
        "**Functions:** `df.describe()`, `df.shape`, `df.dtypes`  \n",
        "**Why:** Identify numerical range issues and inconsistent types.  \n",
        "**Next Step:** Handle missing values.  \n",
        "**Search Rule:** Large variance between mean and median often signals skew.  \n",
        "**Visual:** üìà *Descriptive summary output* ‚Äî table of mean, std, min, max."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deab9c38",
      "metadata": {
        "id": "deab9c38"
      },
      "source": [
        "## **4. Missing Values**\n",
        "**Action:** Detect and address nulls.  \n",
        "**Functions:** `df.isna().sum()`, `df.fillna()`, `df.dropna()`  \n",
        "**Why:** Missing data biases results and can crash models.  \n",
        "**Next Step:** Check for duplicates.  \n",
        "**Search Rule:** Columns with >30% missing require deeper review.  \n",
        "**Visual:** üü¶ *Missingness heatmap*  \n",
        "```python\n",
        "import seaborn as sns\n",
        "sns.heatmap(df.isna(), cbar=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5b9686",
      "metadata": {
        "id": "2b5b9686"
      },
      "source": [
        "## **5. Duplicates**\n",
        "**Action:** Identify and remove duplicate rows.  \n",
        "**Functions:** `df.duplicated().sum()`, `df.drop_duplicates()`  \n",
        "**Why:** Duplicate rows inflate metrics and correlations.  \n",
        "**Next Step:** Separate columns by type (numeric vs categorical).  \n",
        "**Search Rule:** Inspect whether duplicates differ only by timestamp or ID.  \n",
        "**Visual:** üßæ *Before/after row count comparison* ‚Äî simple bar chart of record totals."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b87bddd",
      "metadata": {
        "id": "8b87bddd"
      },
      "source": [
        "## **6. Identify Data Types**\n",
        "**Action:** Classify features (numeric, categorical, datetime).  \n",
        "**Functions:** `df.select_dtypes()`, `pd.to_datetime()`  \n",
        "**Why:** Determines analysis and visualization strategy.  \n",
        "**Next Step:** Run univariate exploration.  \n",
        "**Search Rule:** If few unique values in a numeric column ‚Üí treat as categorical.  \n",
        "**Visual:** üßÆ *Pie or count chart* ‚Äî number of columns by data type."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1f24898",
      "metadata": {
        "id": "f1f24898"
      },
      "source": [
        "## **7. Univariate Exploration**\n",
        "**Action:** Examine each variable‚Äôs distribution.  \n",
        "**Functions:** `.hist()`, `sns.boxplot()`, `.value_counts()`  \n",
        "**Why:** Detect skew, outliers, or dominant categories.  \n",
        "**Next Step:** Investigate outliers more closely.  \n",
        "**Search Rule:** If distribution is right-skewed ‚Üí consider log transform.  \n",
        "**Visual:** üìä *Histogram / boxplot / countplot*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0292345",
      "metadata": {
        "id": "b0292345"
      },
      "source": [
        "## **8. Outlier Detection**\n",
        "**Action:** Quantify extreme points.  \n",
        "**Functions:** `sns.boxplot()`, `df['col'].quantile([0.01, 0.99])`  \n",
        "**Why:** Outliers distort averages and trends.  \n",
        "**Next Step:** Cap, transform, or investigate outliers.  \n",
        "**Search Rule:** If outlier points share a category, they may reveal segmentation insight.  \n",
        "**Visual:** üö® *Boxplot highlighting whiskers and outliers.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0abd7d",
      "metadata": {
        "id": "bf0abd7d"
      },
      "source": [
        "## **9. Numeric vs. Categorical Analysis**\n",
        "**Action:** Compare how numeric variables distribute across categories.  \n",
        "**Functions:** `sns.boxplot(x='category', y='numeric_col', data=df)`, `sns.violinplot(x='category', y='numeric_col', data=df)`, `df.groupby('category')['numeric_col'].mean().sort_values()`  \n",
        "**Why:** Shows how numeric features differ by categorical groups before correlation.  \n",
        "**Next Step:** Identify variables that show strong group separation for encoding or modeling.  \n",
        "**Search Rule:** If category groups have non-overlapping boxes or very different medians, that feature may be a strong differentiator.  \n",
        "**Visual:** üéª *Boxplot or violin plot comparing numeric values across categories.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d10c133",
      "metadata": {
        "id": "9d10c133"
      },
      "source": [
        "## **10. Correlation Analysis**\n",
        "**Action:** Measure numeric relationships.  \n",
        "**Functions:** `df.corr()`, `sns.heatmap()`  \n",
        "**Why:** Identify redundant or predictive variables.  \n",
        "**Next Step:** Remove collinear features or create composite ones.  \n",
        "**Search Rule:** If correlation > 0.85, drop or merge one of the features since this is likely multicollinearity ‚Äî when two or more features carry almost the same information.  \n",
        "**Visual:** üß© *Correlation heatmap with annotations.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "329c6f67",
      "metadata": {
        "id": "329c6f67"
      },
      "source": [
        "## **11. Encoding Categorical Features**\n",
        "**Action:** Convert categorical variables to numeric form.  \n",
        "**Functions:** `pd.get_dummies()`, `LabelEncoder()`  \n",
        "**Why:** Required for correlation or modeling, since most algorithms and statistical tests need numeric input.  \n",
        "**Next Step:** Combine the numeric and newly encoded categorical features into a single analysis-ready dataset, then proceed to Step 12 to explore relationships across all variables.  \n",
        "**Search Rule:** One-hot encode only *moderate-cardinality* features (< 20 unique values) identified in Step 6 to preserve model performance and avoid feature explosion.  \n",
        "**Visual:** üß† *Bar chart comparing encoded category counts.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28308a5b",
      "metadata": {
        "id": "28308a5b"
      },
      "source": [
        "## **12. Bivariate & Multivariate Analysis**\n",
        "**Action:** After combining the numeric and encoded categorical columns from Step 11 explore relationships between variables.\n",
        "- Numeric vs Numeric: correlations, scatterplots, PCA\n",
        "- Categorical vs Numeric: group means, boxplots, ANOVA\n",
        "- Categorical vs Categorical: contingency tables, chi-square tests\n",
        "\n",
        "**Functions:** `sns.scatterplot()`, `sns.boxplot()`, `sns.pairplot()`  \n",
        "**Why:** Reveals patterns, clusters, or separability.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12a. Numeric vs Numeric**\n",
        "\n",
        "**Use when:** both variables are continuous (e.g., income vs age, temperature vs sales).  \n",
        "**Goal:** Detect linear or nonlinear relationships between numeric variables.\n",
        "\n",
        "**Functions**\n",
        "```python\n",
        "# correlation matrix and scatterplots\n",
        "df.corr()\n",
        "sns.scatterplot(x='var1', y='var2', hue='target', data=df)\n",
        "sns.pairplot(df[numeric_cols])\n",
        "\n",
        "# PCA for multivariate pattern detection\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(df[numeric_cols])\n",
        "sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1])\n"
      ],
      "metadata": {
        "id": "X4CBWMpU7fGz"
      },
      "id": "X4CBWMpU7fGz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "What to look for:\n",
        "\n",
        "- |r| ‚â• 0.5 ‚Üí strong linear correlation\n",
        "\n",
        "- Curved or clustered scatterplots ‚Üí possible nonlinear or grouped relationships\n",
        "\n",
        "- PCA revealing clusters ‚Üí variables that move together or separate groups\n",
        "\n",
        "‚úÖ Use when: you suspect continuous variables are related or redundant.\n",
        "\n",
        "Search Rule:\n",
        "If two numeric features have |r| ‚â• 0.85, consider dropping one to avoid multicollinearity,\n",
        "or combining them if they represent similar signals (e.g., total and average metrics)."
      ],
      "metadata": {
        "id": "U8oC5q4g8d4R"
      },
      "id": "U8oC5q4g8d4R"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12b. Categorical vs Numeric**\n",
        "\n",
        "**Use when:** a categorical variable may influence a numeric variable  \n",
        "(e.g., customer segment vs spending, education level vs salary).  \n",
        "**Goal:** Check whether numeric values differ meaningfully across categories.\n",
        "\n",
        "**Functions**\n",
        "```python\n",
        "# visualize differences across categories\n",
        "sns.boxplot(x='category', y='numeric_var', data=df)\n",
        "\n",
        "# two-group comparison\n",
        "from scipy import stats\n",
        "stats.ttest_ind(df[df['category']=='A']['numeric_var'],\n",
        "                df[df['category']=='B']['numeric_var'])\n",
        "\n",
        "# 3+ groups (ANOVA)\n",
        "stats.f_oneway(df[df['category']=='A']['numeric_var'],\n",
        "               df[df['category']=='B']['numeric_var'],\n",
        "               df[df['category']=='C']['numeric_var'])\n"
      ],
      "metadata": {
        "id": "x9iY352t7yRD"
      },
      "id": "x9iY352t7yRD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "What to look for:\n",
        "\n",
        "- Boxplots: IQRs or medians clearly separated, minimal overlap\n",
        "\n",
        "- Statistical: p < 0.05 and Cohen‚Äôs d ‚â• 0.8 ‚Üí strong effect size\n",
        "\n",
        "- Consistent direction (e.g., income increases with education level)\n",
        "\n",
        "‚úÖ Use when: you expect group membership to explain numeric variation.\n",
        "\n",
        "Search Rule:\n",
        "If groups have <30% IQR overlap or p < 0.05 (ANOVA/t-test) with large effect size (d ‚â• 0.8),\n",
        "note that relationship for potential interaction or feature creation."
      ],
      "metadata": {
        "id": "9K9Vy8NF87IM"
      },
      "id": "9K9Vy8NF87IM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12c. Categorical vs Categorical**\n",
        "\n",
        "**Use when:** both features are categorical  \n",
        "(e.g., gender vs churn status, plan type vs region).  \n",
        "**Goal:** Test whether categories are independent or related.\n",
        "\n",
        "**Functions**\n",
        "```python\n",
        "# contingency table and chi-square test\n",
        "pd.crosstab(df['cat1'], df['cat2'])\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "chi2, p, dof, expected = chi2_contingency(pd.crosstab(df['cat1'], df['cat2']))\n",
        "print(f\"Chi-square = {chi2:.2f}, p = {p:.4f}\")\n"
      ],
      "metadata": {
        "id": "rB15lhhy9SkE"
      },
      "id": "rB15lhhy9SkE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "What to look for\n",
        "\n",
        "- Chi-square p < 0.05 ‚Üí the relationship between categories is likely not random\n",
        "\n",
        "- Crosstab shows large differences in counts between categories or unexpected patterns\n",
        "\n",
        "- If p > 0.05 ‚Üí categories are likely independent (no meaningful association)\n",
        "\n",
        "‚úÖ Use when: you want to determine if two categorical features are dependent,\n",
        "such as ‚Äúplan type vs region‚Äù or ‚Äúgender vs churn status.‚Äù\n",
        "\n",
        "Search Rule:\n",
        "If chi-square p < 0.05 or crosstab shows uneven distributions between groups,\n",
        "note the pair for potential feature engineering (e.g., grouped or interaction variable).\n",
        "\n",
        "**Next Step:** Engineer or aggregate features.  "
      ],
      "metadata": {
        "id": "36KZUfjM9W-3"
      },
      "id": "36KZUfjM9W-3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12d. Summary & Transition to Feature Engineering**\n",
        "\n",
        "**Goal:**  \n",
        "Summarize patterns from your bivariate and multivariate exploration to decide  \n",
        "which variables should be engineered, transformed, or reduced before modeling.\n",
        "\n",
        "**Action Steps**\n",
        "1. **Document key insights** from your plots and tests:  \n",
        "   - Which variables had strong correlations (|r| ‚â• 0.8)?  \n",
        "   - Which categorical groups showed strong numeric separation (p < 0.05, d ‚â• 0.8)?  \n",
        "   - Which pairs of variables formed distinct clusters or patterns in PCA/scatterplots?  \n",
        "\n",
        "2. **Flag candidates for feature engineering:**  \n",
        "   - Combine or transform variables that move together (e.g., ratios, log scales).  \n",
        "   - Create interaction features from strongly associated pairs (e.g., income √ó tenure).  \n",
        "   - Consider dropping redundant features with near-perfect correlation.  \n",
        "\n",
        "3. **Decide if dimensionality reduction is needed:**  \n",
        "   - If you have many correlated numeric variables ‚Üí try **PCA**.  \n",
        "   - If you have high-cardinality categorical variables ‚Üí consider **encoding reduction** or **grouping**.  \n",
        "\n",
        "4. **Create a short summary table:**\n",
        "   | Feature | Relationship | Action | Rationale |\n",
        "   |----------|---------------|---------|------------|\n",
        "   | `income` & `tenure` | Moderate positive correlation | Create ratio `income_per_tenure` | Improves normalization across customers |\n",
        "   | `plan_type` & `region` | Chi-square p < 0.05 | Encode interaction | Captures regional plan bias |\n",
        "\n",
        "**Next Step:**\n",
        "Proceed to **Step 13 ‚Äî Feature Engineering Readiness** to implement your chosen transformations.\n",
        "\n",
        "Before moving on, finalize the dataset that will carry into feature engineering:\n",
        "\n",
        "```python\n",
        "# Define the combined, analysis-ready dataset\n",
        "# (contains all numeric + encoded categorical features)\n",
        "df_combined = df.copy()  # or use the merged DataFrame from Step 11\n",
        "\n",
        "# Make a working copy for feature engineering\n",
        "df_features = df_combined.copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "t1R2JoIJ-44O"
      },
      "id": "t1R2JoIJ-44O"
    },
    {
      "cell_type": "markdown",
      "id": "56380429",
      "metadata": {
        "id": "56380429"
      },
      "source": [
        "## **13. Feature Engineering Readiness**\n",
        "\n",
        "**Setup:**  \n",
        "Work from the `df_features` DataFrame created in Step 12d ‚Äî  \n",
        "this is your copy of the fully combined, encoded dataset (`df_combined`) that passed all cleaning and exploratory checks.\n",
        "\n",
        "**Action:** Create, transform, or aggregate features.  \n",
        "**Functions:** `.apply()`, `.groupby()`, ratio and delta columns.  \n",
        "**Why:** Embed business logic or reduce redundancy.   \n",
        "**Search Rule:** Look for transformations that increase correlation or class separation.  \n",
        "**Next Step:** Standardize scales and clean final features.\n",
        "\n",
        "**Visual:** üß∞ *Before/after comparison plot*."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14. Train‚ÄìTest Split**\n",
        "\n",
        "**Action:**  \n",
        "Partition your engineered dataset (`df_features`) into training and test subsets before scaling.\n",
        "\n",
        "**Functions:** `train_test_split()`  \n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_features.drop('target', axis=1)\n",
        "y = df_features['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "DnJ-ImJICNk5"
      },
      "id": "DnJ-ImJICNk5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why:\n",
        "Prevents data leakage ‚Äî all scaling and model fitting will occur using only the training data.\n",
        "\n",
        "Search Rule:\n",
        "Confirm both splits have similar class distributions and column shapes.\n",
        "\n",
        "Visual: üìä Bar chart comparing class proportions in train vs test.\n",
        "\n",
        "Next Step:\n",
        "Scale or normalize features (Step 15) using only the training subset."
      ],
      "metadata": {
        "id": "gA4F1fwGCQNV"
      },
      "id": "gA4F1fwGCQNV"
    },
    {
      "cell_type": "markdown",
      "id": "3acb6c86",
      "metadata": {
        "id": "3acb6c86"
      },
      "source": [
        "## **15. Data Cleaning / Transformation**\n",
        "\n",
        "**Setup:**  \n",
        "Start from the split datasets created in Step 14 (`X_train`, `X_test`, `y_train`, `y_test`).\n",
        "\n",
        "---\n",
        "\n",
        "**Action:**  \n",
        "1. **Check for missing values**  \n",
        "2. **Impute or fill NA values**  \n",
        "3. **Normalize and standardize numeric features**\n",
        "\n",
        "**Functions:** `.isnull()`, `.fillna()`, `SimpleImputer()`, `StandardScaler()`, `.replace()`, `.str.lower()`\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 1Ô∏è‚É£ Check for nulls\n",
        "print(X_train.isnull().sum().sort_values(ascending=False).head())\n",
        "\n",
        "# 2Ô∏è‚É£ Fill or impute missing values\n",
        "# Numeric features: median imputation\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X_train_imputed = pd.DataFrame(num_imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_imputed  = pd.DataFrame(num_imputer.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "# Optional: categorical imputation if any\n",
        "# cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# 3Ô∏è‚É£ Standardize numeric features (fit only on train)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=X_train.columns)\n",
        "X_test_scaled  = pd.DataFrame(scaler.transform(X_test_imputed), columns=X_test.columns)\n",
        "\n",
        "# 4Ô∏è‚É£ (Optional) String cleanup for text features\n",
        "# df['column'] = df['column'].str.lower().str.strip()\n",
        "\n",
        "# 5Ô∏è‚É£ Final error & quality check\n",
        "print(\"\\nFinal QA check:\")\n",
        "print(\"Any NaNs left? \", X_train_scaled.isnull().any().any() or X_test_scaled.isnull().any().any())\n",
        "print(\"Any infinite values? \", np.isinf(X_train_scaled.values).any() or np.isinf(X_test_scaled.values).any())\n",
        "\n",
        "# Check for nonsensical or extreme values\n",
        "desc = X_train_scaled.describe()\n",
        "print(\"\\nFeature summary (post-scaling):\")\n",
        "print(desc.T[['mean', 'std', 'min', 'max']].round(3))\n",
        "\n",
        "# Example logic check for specific columns\n",
        "if 'age' in X_train_scaled.columns:\n",
        "    invalid_age_count = (X_train_scaled['age'] < 0).sum()\n",
        "    print(f\"Invalid ages (<0): {invalid_age_count}\")\n",
        "\n",
        "# 6Ô∏è‚É£ Validate numeric distributions and feature shapes\n",
        "\n",
        "# Check that the scaled data still has the same shape and columns\n",
        "print(\"\\nShape comparison:\")\n",
        "print(f\"Before scaling: {X_train.shape}, After scaling: {X_train_scaled.shape}\")\n",
        "print(f\"Columns match? {list(X_train.columns) == list(X_train_scaled.columns)}\")\n",
        "\n",
        "# Quick visual distribution sanity check\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "num_cols = X_train_scaled.columns[:6]  # sample a few for readability\n",
        "fig, axes = plt.subplots(len(num_cols), 2, figsize=(10, len(num_cols)*2))\n",
        "for i, col in enumerate(num_cols):\n",
        "    sns.histplot(X_train[col], ax=axes[i,0], kde=True, color='gray')\n",
        "    axes[i,0].set_title(f\"{col} ‚Äî Before Scaling\")\n",
        "    sns.histplot(X_train_scaled[col], ax=axes[i,1], kde=True, color='blue')\n",
        "    axes[i,1].set_title(f\"{col} ‚Äî After Scaling\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why:\n",
        "\n",
        "- Prevents model errors due to NaNs.\n",
        "\n",
        "- Ensures features share comparable scales and consistent text formatting.\n",
        "\n",
        "- Keeps transformations leakage-free by fitting imputers/scalers only on training data.\n",
        "\n",
        "Search Rule:\n",
        "\n",
        "- no NaNs or inf values remain,\n",
        "\n",
        "- numeric distributions look reasonable,\n",
        "\n",
        "- shape and feature counts match pre-scaling versions.\n",
        "\n",
        "Next Step: Export cleaned, model-ready data (Step 16).\n",
        "\n",
        "Visual: ‚öñÔ∏è Histogram before vs after imputation / scaling to verify normalization."
      ],
      "metadata": {
        "id": "FepId6AvDf-C"
      },
      "id": "FepId6AvDf-C"
    },
    {
      "cell_type": "markdown",
      "id": "546ace6d",
      "metadata": {
        "id": "546ace6d"
      },
      "source": [
        "## **16. Data Export**\n",
        "(Optional: export only if you plan to reload the data later or share with other tools.)\n",
        "**Setup:**  \n",
        "Use the final cleaned and scaled datasets produced in Step 15:  \n",
        "`X_train_scaled`, `X_test_scaled`, `y_train`, and `y_test`.\n",
        "\n",
        "---\n",
        "\n",
        "**Action:**  \n",
        "Save your model-ready data for reproducibility and future modeling.  \n",
        "Perform a lightweight final verification (shapes + null counts) before export.\n",
        "\n",
        "**Functions:** `to_csv()`, `to_pickle()`, `.shape`, `.isnull().sum()`\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# ‚úÖ 1Ô∏è‚É£ Final quick verification\n",
        "print(\"Final dataset shapes:\")\n",
        "print(f\"X_train_scaled: {X_train_scaled.shape}\")\n",
        "print(f\"X_test_scaled : {X_test_scaled.shape}\")\n",
        "print(f\"y_train       : {y_train.shape}\")\n",
        "print(f\"y_test        : {y_test.shape}\")\n",
        "\n",
        "print(\"\\nNull value summary:\")\n",
        "print(f\"Train features nulls: {X_train_scaled.isnull().sum().sum()}\")\n",
        "print(f\"Test features nulls : {X_test_scaled.isnull().sum().sum()}\")\n",
        "\n",
        "# Optional: raise a quick alert if anything unexpected appears\n",
        "if X_train_scaled.isnull().any().any() or X_test_scaled.isnull().any().any():\n",
        "    print(\"‚ö†Ô∏è Warning: Nulls detected ‚Äî revisit Step 15 before exporting.\")\n",
        "\n",
        "# ‚úÖ 2Ô∏è‚É£ Export to CSV for reproducibility\n",
        "X_train_scaled.to_csv('X_train_scaled.csv', index=False)\n",
        "X_test_scaled.to_csv('X_test_scaled.csv', index=False)\n",
        "y_train.to_csv('y_train.csv', index=False)\n",
        "y_test.to_csv('y_test.csv', index=False)\n",
        "\n",
        "# ‚úÖ 3Ô∏è‚É£ (Optional) Save as pickles for faster reload\n",
        "# X_train_scaled.to_pickle('X_train_scaled.pkl')\n",
        "# X_test_scaled.to_pickle('X_test_scaled.pkl')\n",
        "# y_train.to_pickle('y_train.pkl')\n",
        "# y_test.to_pickle('y_test.pkl')\n",
        "\n",
        "# ‚úÖ 4Ô∏è‚É£ Preview confirmation\n",
        "print(\"\\nPreview of exported training data:\")\n",
        "display(X_train_scaled.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why:\n",
        "Ensures all processed datasets are safely stored and reproducible for later modeling,\n",
        "without re-running expensive or redundant validation.\n",
        "\n",
        "Next Step:\n",
        "Move into Step 17 ‚Äì Model Training and Evaluation using these exported datasets.\n",
        "\n",
        "Search Rule:\n",
        "Verify exported files exist and match expected dimensions.\n",
        "(Optional: use os.path.getsize() to confirm write success.)\n",
        "\n",
        "Visual: üíæ Side-by-side preview ‚Äî raw vs cleaned dataset."
      ],
      "metadata": {
        "id": "XSysi4TBHnCu"
      },
      "id": "XSysi4TBHnCu"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-svYeAUYHhGT"
      },
      "id": "-svYeAUYHhGT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}