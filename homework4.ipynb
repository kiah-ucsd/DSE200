{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACBapk1J0llK"
      },
      "source": [
        "# Movie Review Sentiment Analysis and Rating Prediction\n",
        "\n",
        "In this homework, you will:\n",
        "1. Load IMDB movie reviews dataset using Hugging Face datasets\n",
        "2. Perform sentiment analysis\n",
        "3. Build a ML model to predict movie ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "KFMzfbGT0llM"
      },
      "source": [
        "# TODO: Install required packages\n",
        "%pip install pandas numpy scikit-learn transformers torch datasets"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8PpsGpk0llN"
      },
      "source": [
        "# TODO: Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "from transformers import pipeline\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "# Add any other libraries you need"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58qs69wI0llN"
      },
      "source": [
        "## Part 1: Load Dataset\n",
        "\n",
        "Load the IMDB dataset using Hugging Face datasets library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7wRRK5X0llN"
      },
      "source": [
        "# TODO: Load the IMDB dataset\n",
        "# Hint: Use load_dataset('imdb')\n",
        "imdb_dataset = load_dataset('imdb')\n",
        "# Convert to pandas DataFrame for easier manipulation"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkICAJC40llO"
      },
      "source": [
        "## Part 2: Data Preprocessing\n",
        "\n",
        "Clean and prepare the text data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = imdb_dataset['test'].to_pandas()\n",
        "train_df = imdb_dataset['train'].to_pandas()"
      ],
      "metadata": {
        "id": "vxS5czPL3W43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jHwBQMN74tg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFmmWqti0llO"
      },
      "source": [
        "# TODO: Create a function to clean text\n",
        "def clean_text(text):\n",
        "  # 1. Remove HTML tags\n",
        "  text = re.sub(r'<.*?>', '', text)\n",
        "  # 2. Remove special characters\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s]','', text)\n",
        "  # 3. Convert to lowercase\n",
        "  text = text.lower()\n",
        "  return text\n",
        "test_df['clean_text'] = test_df['text'].apply(clean_text)\n",
        "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
        "# Hint: Use regular expressions"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "Ujf5NdmG6umz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YNR3JaX0llO"
      },
      "source": [
        "## Part 3: Advanced Sentiment Analysis\n",
        "\n",
        "Go beyond binary classification - use a pre-trained model to get continuous sentiment scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if I can run a gpu to speed things up\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "Wk1Kjum1M3Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no gpu available so we'll batch instead\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "iguDY8X4dkOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TODO: Implement advanced sentiment analysis\n",
        "# 1. Load a pre-trained model (hint: try 'distilbert-base-uncased-finetuned-sst-2-english')\n",
        "# create the pipline using sentiment-analysis and the suggested model\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    device=0  # CPU fallback if no GPU\n",
        ")\n",
        "\n",
        "# 2. Create a function to get continuous sentiment scores\n",
        "def batch_sentiment_scores(\n",
        "    texts,\n",
        "    batch_size=10,\n",
        "    save_every=10,\n",
        "    save_path = \"/content/drive/MyDrive/sentiment_progress.csv\"\n",
        "):\n",
        "    scores = []\n",
        "    if os.path.exists(save_path):\n",
        "        saved = pd.read_csv(save_path)\n",
        "        scores = saved[\"score\"].tolist()\n",
        "        print(f\"Resuming from {len(scores)} saved scores...\")\n",
        "    else:\n",
        "        print(\"Starting fresh...\")\n",
        "\n",
        "    start_index = len(scores)\n",
        "    print(f\"Processing {len(texts)} texts in batches of {batch_size}...\")\n",
        "\n",
        "    for i in tqdm(range(start_index, len(texts), batch_size)):\n",
        "        batch_start = time.time()\n",
        "        batch = texts[i:i+batch_size].tolist()\n",
        "        results = sentiment_pipeline(batch, truncation=True)\n",
        "\n",
        "        # Flip the sign if the label is negative\n",
        "        for r in results:\n",
        "            score = r[\"score\"]\n",
        "            if r[\"label\"] == \"NEGATIVE\":\n",
        "                score = -score\n",
        "            scores.append(score)\n",
        "\n",
        "        # Save progress every N batches or at the end\n",
        "        if (i // batch_size + 1) % save_every == 0 or i + batch_size >= len(texts):\n",
        "            pd.DataFrame({\n",
        "                \"row_id\": list(range(len(scores))),\n",
        "                \"score\": scores\n",
        "            }).to_csv(save_path, index=False)\n",
        "            print(f\"Progress saved at {save_path} ({len(scores)} scores).\")\n",
        "\n",
        "        print(f\"Batch {i // batch_size + 1} completed in {time.time() - batch_start:.2f} sec\")\n",
        "\n",
        "    print(\"âœ… All batches complete. Final results saved.\")\n",
        "    return np.array(scores)\n",
        "\n",
        "# 3. Apply it to your cleaned text data\n",
        "test_df[\"score\"] = batch_sentiment_scores(\n",
        "    test_df[\"clean_text\"],\n",
        "    batch_size=10,\n",
        "    save_every=10\n",
        ")\n",
        "\n",
        "# Optional smaller sample test\n",
        "# sample_df = test_df.sample(250, random_state=42).reset_index(drop=True)\n",
        "# sample_df[\"score\"] = batch_sentiment_scores(sample_df[\"clean_text\"], batch_size=10)\n"
      ],
      "metadata": {
        "id": "Ljq0DrRVahKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sentiment_progress.csv\")"
      ],
      "metadata": {
        "id": "r3R_bO2IV-cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "test_df = pd.read_csv(\"sentiment_progress.csv\")"
      ],
      "metadata": {
        "id": "NLvjWKxnXgns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1s1KQPj0llP"
      },
      "source": [
        "## Part 4: Feature Engineering\n",
        "\n",
        "Create rich features for your model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.head()"
      ],
      "metadata": {
        "id": "LpY2voVfQtcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_JFE-bQ0llP"
      },
      "source": [
        "# TODO: Create features\n",
        "# 1. Use your continuous sentiment scores\n",
        "\n",
        "# 2. Calculate text statistics:\n",
        "#    - Length\n",
        "#    - Word count\n",
        "#    - Average word length\n",
        "#    - Sentence count\n",
        "# 3. Any other features you think might help!"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThOFkl_P0llP"
      },
      "source": [
        "## Part 5: Multi-Class Rating Prediction\n",
        "\n",
        "Instead of binary classification, predict a 5-star rating!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orPEDEfj0llP"
      },
      "source": [
        "# TODO: Create target variable\n",
        "# Convert binary labels to 5-star ratings using your features\n",
        "# Hint: Use sentiment scores and other features to estimate star rating"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcvcLBc00llQ"
      },
      "source": [
        "# TODO: Build and train your model\n",
        "# 1. Split data into train and test sets\n",
        "# 2. Choose a model suitable for multi-class classification\n",
        "# 3. Train the model\n",
        "# 4. Make predictions\n",
        "# 5. Evaluate performance"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs8kGBsX0llQ"
      },
      "source": [
        "## Part 6: Analysis\n",
        "\n",
        "Analyze your results and suggest improvements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i347E4Ou0llQ"
      },
      "source": [
        "# TODO: Create visualizations and analyze:\n",
        "# 1. Confusion matrix for multi-class predictions\n",
        "# 2. Feature importance\n",
        "# 3. Error analysis\n",
        "# 4. Suggest improvements"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}